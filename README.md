# ğŸŒŠ OceanAI PDFâ€¯RAG Assistant

A lightweight multimodal **Retrievalâ€‘Augmented Generation (RAG)** application that lets you ask both **textualâ€¯andâ€¯visual questions** about the contents of one or more PDF documents.

Built with **Streamlit** for UI and **Ollama** local models for fast, private inference.

---

## âœ¨ Key Features

| Capability                 | Details                                                                                 |
| -------------------------- | --------------------------------------------------------------------------------------- |
| ğŸ“„ **Multiâ€‘PDF ingestion** | Upload any number of PDF files in the sidebar.                                          |
| ğŸ” **Hybrid parsing**      | â€¢ Extract native textÂ â€¢ Run OCR (Tesseract) on scanned pagesÂ â€¢ Capture embedded images. |
| ğŸ§© **Vector search**       | Text chunks embedded with **nomicâ€‘embedâ€‘text** â†’ stored in a local **Chroma** DB.       |
| ğŸ¤– **Text QA**             | Questions answered with the 8â€‘bit **mistral** model via `langchain_ollama`.             |
| ğŸ–¼ **Image QA**            | Ask about figures/tables using **llava:7b** (multimodal) â€” one click per image.         |
| ğŸ’¾ **Offlineâ€‘friendly**    | All models run locally under **Ollama**; no OpenAI key required.                        |

---

## ğŸš€ QuickÂ Start

> **Prerequisites**
> â€¢ PythonÂ â‰¥â€¯3.9Â 
> â€¢ [Tesseractâ€‘OCR](https://github.com/tesseract-ocr/tesseract) installed and on yourÂ `PATH`
> â€¢ [Ollama](https://ollama.com) installed and running (`ollama serve`)

```bash
# 1ï¸âƒ£  Clone the repo
$ git clone https://github.com/YOUR_USERNAME/oceanaiâ€‘ragâ€‘assistant.git
$ cd oceanaiâ€‘ragâ€‘assistant

# 2ï¸âƒ£  Install Python deps
$ pip install -r requirements.txt

# 3ï¸âƒ£  Pull the required models (â‰ˆ3â€¯GB total)
$ ollama pull mistral
$ ollama pull llava:7b
$ ollama pull nomic-embed-text

# 4ï¸âƒ£  Launch the app
$ streamlit run streamlit_multimodal_ollama_app.py
```

Then open the local URL shown in your terminal (typically [http://localhost:8501](http://localhost:8501)).

---

## ğŸ–¼ï¸ Screenshots

| Upload & Initialise | Text QA         | Image QA        |
| ------------------- | --------------- | --------------- |
| *(placeholder)*     | *(placeholder)* | *(placeholder)* |

Add PNGs (e.g.Â `assets/screenshot_*.png`) to replace the placeholders.

---

## ğŸ—‚ï¸ FolderÂ Structure

```text
oceanaiâ€‘ragâ€‘assistant/
â”œâ”€â”€ streamlit_multimodal_ollama_app.py   # ğŸ“Œ Main Streamlit entry point
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ README.md                           # â†’ you are here
â”œâ”€â”€ assets/                             # Screenshots & logos (optional)
â”œâ”€â”€ chroma_db/                          # Persisted vector store (autoâ€‘created)
â””â”€â”€ .gitignore                          # Python, Streamlit & editor artefacts
```

---

## ğŸ”§ Configuration

| Variable      | Purpose                      | Default       |
| ------------- | ---------------------------- | ------------- |
| `CHROMA_PATH` | Persisted vector DB location | `./chroma_db` |
| `NUM_CHUNKS`  | Retriever `k` value          | `4`           |

Edit values at the top of `streamlit_multimodal_ollama_app.py` as needed.

---

## ğŸ“– HowÂ ItÂ Works

1. **Ingestion**Â â€” Each uploaded PDF is temporarily saved, then processed pageâ€‘byâ€‘page with *PyMuPDF* (`fitz`).
2. **Extraction**Â â€” For every page:

   * Native text â†’ added as a `Document` (typeÂ =`text`).
   * Images â†’ OCR via *pytesseract* â†’ (typeÂ =`ocr`).
   * Embedded images also saved & referenced (typeÂ =`image`).
3. **Chunking & Embedding**Â â€” Nonâ€‘image docs are chunked (1â€¯000â€¯chars, 20â€¯% overlap) and embedded with **nomicâ€‘embedâ€‘text**.
4. **Vector Store**Â â€” Chunks go into a **Chroma** DB; the DB is cached with `st.cache_resource`.
5. **RetrievalÂ +Â Generation**Â â€”

   * **Text tab**: Retriever â†’ `mistral` LLama for answer + sources (via `langchain` `RetrievalQA`).
   * **Image tab**: Selected figure â†’ base64 â†’ prompt sent to **llava:7b** (`OllamaLLM`) together with your question.

---

## ğŸ§© TechÂ Stack

| Layer         | Library / Tool               | Why                                  |
| ------------- | ---------------------------- | ------------------------------------ |
| UI            | **Streamlit**                | Rapid dataâ€‘app prototyping           |
| PDF handling  | **PyMuPDF (fitz)**           | Fast page parsing & image extraction |
| OCR           | **pytesseract**              | Reliable openâ€‘source OCR             |
| Embeddings    | **nomicâ€‘embedâ€‘text**         | 768â€‘dim, licenceâ€‘friendly            |
| Vector DB     | **Chroma**                   | Lightweight, local vector store      |
| LLMs & Images | **OllamaÂ +Â LLaVA / Mistral** | Fully local inference                |
| Orchestration | **LangChain**                | Chains, retrievers & agents          |

---

## ğŸ“‹ Requirements.txt

```
streamlit>=1.35
pymupdf
pillow
pytesseract
langchain
langchain_ollama
chromadb
```

Tesseractâ€‘OCR system package must also be installed (e.g.Â `sudo apt install tesseract-ocr` on Ubuntu).

---

## ğŸš§ Roadmap / FutureÂ Work

* [ ] **Streaming responses** with Serverâ€‘Sent Events.
* [ ] **Multiâ€‘format ingestion** (DOCX, PPTX, webpages).
* [ ] **Better image handling**Â â€” highlight bounding boxes inâ€‘place.
* [ ] **Deploy** to Streamlit Cloud using custom Docker w/ Ollama.

Contributions via pull requests or issues are welcome!

---

## ğŸ“ License

Distributed under the MIT License. See `LICENSE` for details.

---

## ğŸ™‹â€â™‚ï¸ Author

|                  |                                                                       |
| ---------------- | --------------------------------------------------------------------- |
| **Harisankarâ€¯M** | [https://github.com/YOUR\_USERNAME](https://github.com/YOUR_USERNAME) |
